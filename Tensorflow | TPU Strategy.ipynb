{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPU Strategy\n\n**TPUStrategy**\n\ntf.distribute.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. They are available on Google Colab, the TPU Research Cloud, and Cloud TPU.\n\nIn terms of distributed training architecture, TPUStrategy is the same MirroredStrategy‚Äîit implements synchronous distributed training. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in TPUStrategy.\n","metadata":{}},{"cell_type":"markdown","source":"# Imports Libs ‚öíÔ∏è ‚öôÔ∏è","metadata":{}},{"cell_type":"code","source":"# Import Required Libs , TensorFlow ,and TensorFlow Datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os, time\nimport numpy as np\nfrom IPython.display import HTML,display\n\ntry:\n  # %tensorflow_version only exists .\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\n\ntfds.disable_progress_bar()","metadata":{"id":"TkUjfmKkflCd","execution":{"iopub.status.busy":"2023-05-11T13:20:12.343094Z","iopub.execute_input":"2023-05-11T13:20:12.344145Z","iopub.status.idle":"2023-05-11T13:20:12.351596Z","shell.execute_reply.started":"2023-05-11T13:20:12.344111Z","shell.execute_reply":"2023-05-11T13:20:12.350800Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data ‚åõ","metadata":{}},{"cell_type":"markdown","source":"`TFDS` provides a collection of ready-to-use datasets for use with TensorFlow, and other Machine Learning frameworks.\n\nIt handles downloading and preparing the data deterministically and constructing a tf.data.Dataset (or np.array).\n\nWe are going to use the `fashion_mnist` dataset  which is only split into a `TRAINING` set. We have to use tfds.splits to split this training set into to a training_set, a validation_set, and a test_set. \n\nIn this example we are splitting:\n- Train data \n- Test data","metadata":{}},{"cell_type":"code","source":"# Load the Fashion-MNIST dataset\n(train_images, train_labels), (test_images, test_labels)= tf.keras.datasets.fashion_mnist.load_data()\n","metadata":{"id":"eQN-PtIGgFtH","execution":{"iopub.status.busy":"2023-05-11T13:20:15.499345Z","iopub.execute_input":"2023-05-11T13:20:15.499703Z","iopub.status.idle":"2023-05-11T13:20:15.922627Z","shell.execute_reply.started":"2023-05-11T13:20:15.499677Z","shell.execute_reply":"2023-05-11T13:20:15.921776Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Explore & visualize Data  üîç üìä üëÄ","metadata":{}},{"cell_type":"code","source":"# Get the number of examples in each set from the dataset info.\nprint('Total Number of Training Images: {}'.format(len(train_images)))\nprint('Total Number of Test Images: {} \\n'.format(len(test_images)))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:17.126355Z","iopub.execute_input":"2023-05-11T13:20:17.127296Z","iopub.status.idle":"2023-05-11T13:20:17.132372Z","shell.execute_reply.started":"2023-05-11T13:20:17.127264Z","shell.execute_reply":"2023-05-11T13:20:17.131446Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Total Number of Training Images: 60000\nTotal Number of Test Images: 10000 \n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the class names from the dataset\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nnum_classes = len(class_names)\n\nprint(class_names)\nprint('Total Number of Classes: {}'.format(num_classes))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:18.089872Z","iopub.execute_input":"2023-05-11T13:20:18.090778Z","iopub.status.idle":"2023-05-11T13:20:18.096180Z","shell.execute_reply.started":"2023-05-11T13:20:18.090746Z","shell.execute_reply":"2023-05-11T13:20:18.095238Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nTotal Number of Classes: 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Process Data  üëÄ üßê","metadata":{}},{"cell_type":"code","source":"train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\ntest_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:19.378606Z","iopub.execute_input":"2023-05-11T13:20:19.378973Z","iopub.status.idle":"2023-05-11T13:20:19.506275Z","shell.execute_reply.started":"2023-05-11T13:20:19.378947Z","shell.execute_reply":"2023-05-11T13:20:19.505230Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Strategy üëÄ üßê","metadata":{}},{"cell_type":"markdown","source":"Now, you define `strategy` using the `TPUtrategy()` class. \n\n**Note:** \n- If you are running this in Kaggle OT COLAB, make sure you have selected your `Accelerator` to be `TPU` for it to detect it. \n- 8 devices are available.  \n","metadata":{}},{"cell_type":"markdown","source":"- **TPUStrategy**\n- **Running [[ TPU  ]] Mode on Kaggle**\n- **Providing `8 Accelerators`**","metadata":{}},{"cell_type":"code","source":"# # Define the strategy to use and print the number of devices found\ntry:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    # print('Running on TPU ', tpu.cluster_spec().as_dict()['worker']) \n    print ('Number of devices/accelerators: {}'.format(strategy.num_replicas_in_sync))\n\nexcept ValueError:\n  print('TPU failed to initialize.')","metadata":{"id":"eCsDqWnDgNHr","execution":{"iopub.status.busy":"2023-05-11T13:24:47.835348Z","iopub.execute_input":"2023-05-11T13:24:47.836069Z","iopub.status.idle":"2023-05-11T13:24:53.471790Z","shell.execute_reply.started":"2023-05-11T13:24:47.836037Z","shell.execute_reply":"2023-05-11T13:24:53.470494Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Finished initializing TPU system.\nWARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Found TPU system:\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Workers: 1\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"},{"name":"stdout","text":"Number of devices/accelerators: 8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**P.Süí°** Now, we create training and eval examples, define `batch size` and also define `BATCH_SIZE_PER_REPLICA` which is the distribution we are making for each available device. ‚åõ","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE_PER_REPLICA = 128\n# Use for Mirrored Strategy\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n\n# Use for No Strategy , when we run on a singel machine\n# BATCH_SIZE = BATCH_SIZE_PER_REPLICA * 1\n\nprint ('BATCH_SIZE_PER_REPLICA',BATCH_SIZE_PER_REPLICA )\nprint ('GLOBAL_BATCH_SIZE on the machine (BATCH_SIZE_PER_REPLICA * No. of GPUs) =',GLOBAL_BATCH_SIZE )","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:40.798493Z","iopub.execute_input":"2023-05-11T13:20:40.799378Z","iopub.status.idle":"2023-05-11T13:20:40.804681Z","shell.execute_reply.started":"2023-05-11T13:20:40.799339Z","shell.execute_reply":"2023-05-11T13:20:40.803632Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"BATCH_SIZE_PER_REPLICA 128\nGLOBAL_BATCH_SIZE on the machine (BATCH_SIZE_PER_REPLICA * No. of GPUs) = 1024\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(GLOBAL_BATCH_SIZE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:43.080030Z","iopub.execute_input":"2023-05-11T13:20:43.080908Z","iopub.status.idle":"2023-05-11T13:20:43.256773Z","shell.execute_reply.started":"2023-05-11T13:20:43.080873Z","shell.execute_reply":"2023-05-11T13:20:43.255839Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Bulid Model  ‚öôÔ∏èüèóÔ∏è ","metadata":{}},{"cell_type":"markdown","source":"- For model to follow the strategy, we need to define the model within the strategy's scope using `with strategy.scope():` \n\n- The important thing to notice and compare is the time taken for each `epoch` to complete.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(10)\n    ])\n    \n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],steps_per_execution=32)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:20:52.868815Z","iopub.execute_input":"2023-05-11T13:20:52.869612Z","iopub.status.idle":"2023-05-11T13:20:53.072156Z","shell.execute_reply.started":"2023-05-11T13:20:52.869576Z","shell.execute_reply":"2023-05-11T13:20:53.071243Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Train Model üî• üå°Ô∏è\n\nLet the Magic Begin !üîÆ","metadata":{}},{"cell_type":"code","source":"\nwith strategy.scope():\n    EPOCHS = 10\n    start = time.time()\n    model.fit(train_dataset, epochs=EPOCHS )\n    end = time.time()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T13:23:14.657810Z","iopub.execute_input":"2023-05-11T13:23:14.658766Z","iopub.status.idle":"2023-05-11T13:23:27.309982Z","shell.execute_reply.started":"2023-05-11T13:23:14.658731Z","shell.execute_reply":"2023-05-11T13:23:27.308823Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.1359 - accuracy: 0.3497\nEpoch 2/10\n59/59 [==============================] - 0s 8ms/step - loss: 2.3025 - accuracy: 0.1853\nEpoch 3/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 4/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 5/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 6/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 7/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 8/10\n59/59 [==============================] - 0s 8ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 9/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\nEpoch 10/10\n59/59 [==============================] - 0s 7ms/step - loss: 2.3025 - accuracy: 0.1852\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**P.S.üí°** The `time` library is being utilized to estimate the duration of the model training process. Specifically, the number of `epochs` is set to `10`, ","metadata":{}},{"cell_type":"code","source":"duration = round( end - start , 2) \ndisplay(HTML(f\"<h5><b >The duration required for the model to train using TPU Mirrored Strategy : </b> <b style='color:red'>{duration} Seconds üßê  ‚ú®. </b></h5>\"))","metadata":{"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h5><b >The duration required for the model to train using TPU Mirrored Strategy : </b> <b style='color:red'>12.65 Seconds üßê  ‚ú®. </b></h5>"},"metadata":{}}]}]}